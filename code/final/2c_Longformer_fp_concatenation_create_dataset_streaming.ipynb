{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aahFKxqoVkWN"
      },
      "source": [
        "# Longformer Concatenation\n",
        "- To Dos:\n",
        "    - extract remaining embeddings from MASK longformer; determine max size\n",
        "    - train longformer on NER and extract embeddings\n",
        "    - concatenated vectors and train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ybmc_vmJVfyG"
      },
      "outputs": [],
      "source": [
        "!pip install -q datasets\n",
        "!pip install -q evaluate\n",
        "!pip install -q seqeval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NZlp4EFV2G8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import logging\n",
        "import numpy as np\n",
        "from transformers import LongformerForTokenClassification, Trainer, TrainingArguments\n",
        "from datasets import load_from_disk, Dataset, DatasetDict\n",
        "from datasets import Features, ClassLabel, Value\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import evaluate\n",
        "\n",
        "# logging.basicConfig(level=logging.INFO)\n",
        "# logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DtZ0U2zV3oW"
      },
      "outputs": [],
      "source": [
        "# global variables\n",
        "model_name = 'baseline_final_2.5e-5_linear_warmup_11_25' # update to select the right path\n",
        "\n",
        "task = 'ner'\n",
        "size = 'mini'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5_EBCVQV5Su",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4820e71a-013b-40c4-e87c-3c7fdab66458"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# use for google colab\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "# path = '/content/drive/MyDrive/Colab Notebooks/266 Project'\n",
        "path = '/content/drive/MyDrive/Colab Notebooks/DATASCI 266/266 project'\n",
        "path_model = f'{path}/models/{model_name}/model'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxrbC2n3V6fm"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VYAJBd4V8Aa"
      },
      "outputs": [],
      "source": [
        "# general functions\n",
        "def select_data(split, task, size):\n",
        "    \"\"\"\n",
        "    Loads the appropriate dataset per folder structure here: https://drive.google.com/drive/folders/1C3h3rXdbr9nVAC3_G_I-72DfKNiDU_Pa\n",
        "    Input:\n",
        "        Split: ['train', 'val', 'test']\n",
        "        Task: ['ner', 'mask', 'both']\n",
        "        Size: ['testing', 'mini', 'full']\n",
        "    Returns:\n",
        "        Huggingface dataset\n",
        "    \"\"\"\n",
        "    if split not in ['train', 'val', 'test']:\n",
        "        raise ValueError(\"Split value must be in ['train', 'val', 'test']\")\n",
        "    if task not in ['ner', 'mask', 'both', 'binary']:\n",
        "        raise ValueError(\"Task value must be in ['ner', 'mask', 'both']\")\n",
        "    if size not in ['testing', 'mini', 'full']:\n",
        "        raise ValueError(\"Size value must be in ['testing', 'mini', 'full']\")\n",
        "\n",
        "    path_label = {'both': 'longformer', 'ner': 'longformer_ner', 'mask': 'longformer_mask', 'binary': 'longformer_binary'}\n",
        "    # path_label = {'both': 'longformer', 'ner': 'longformer_ner', 'mask': 'longformer_4096'}\n",
        "\n",
        "    if size == 'testing':\n",
        "        ds = load_from_disk(f'{path}/data/tab/{path_label[task]}/lf_{split}_testing')\n",
        "    if size == 'mini':\n",
        "        if split == 'train':\n",
        "            ds = load_from_disk(f'{path}/data/tab/{path_label[task]}/lf_{split}_400')\n",
        "        else:\n",
        "            ds = load_from_disk(f'{path}/data/tab/{path_label[task]}/lf_{split}_50')\n",
        "    if size == 'full':\n",
        "        ds = load_from_disk(f'{path}/data/tab/{path_label[task]}/lf_{split}')\n",
        "\n",
        "    return ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u69WmUnfWAyj"
      },
      "source": [
        "# Load/Save Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Av_4kbIs01UX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "2b52a678-8317-458a-ab85-a6ade6c5c98a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-07705a7e751c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mds_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ner'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'testing'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# step not needed as can specify keys in batch below\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m dataset = Dataset.from_dict({\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mds_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-1d1c9d0bcd5d>\u001b[0m in \u001b[0;36mselect_data\u001b[0;34m(split, task, size)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'testing'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_from_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{path}/data/tab/{path_label[task]}/lf_{split}_testing'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'mini'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_from_disk\u001b[0;34m(dataset_path, keep_in_memory, storage_options)\u001b[0m\n\u001b[1;32m   2214\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_in_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_in_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2215\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposixpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATASETDICT_JSON_FILENAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2216\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDatasetDict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_in_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_in_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2217\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2218\u001b[0m         raise FileNotFoundError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/dataset_dict.py\u001b[0m in \u001b[0;36mload_from_disk\u001b[0;34m(dataset_dict_path, keep_in_memory, storage_options)\u001b[0m\n\u001b[1;32m   1343\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplits\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1344\u001b[0m             \u001b[0mdataset_dict_split_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mposixpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munstrip_protocol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_dict_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1345\u001b[0;31m             dataset_dict[k] = Dataset.load_from_disk(\n\u001b[0m\u001b[1;32m   1346\u001b[0m                 \u001b[0mdataset_dict_split_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_in_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_in_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mload_from_disk\u001b[0;34m(dataset_path, keep_in_memory, storage_options)\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1695\u001b[0m         arrow_table = concat_tables(\n\u001b[0;32m-> 1696\u001b[0;31m             thread_map(\n\u001b[0m\u001b[1;32m   1697\u001b[0m                 \u001b[0mtable_cls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1698\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mposixpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdest_dataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_file\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"filename\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_data_files\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/contrib/concurrent.py\u001b[0m in \u001b[0;36mthread_map\u001b[0;34m(fn, *iterables, **tqdm_kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \"\"\"\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mconcurrent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfutures\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mThreadPoolExecutor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_executor_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mThreadPoolExecutor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0miterables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtqdm_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/contrib/concurrent.py\u001b[0m in \u001b[0;36m_executor_map\u001b[0;34m(PoolExecutor, fn, *iterables, **tqdm_kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m         with PoolExecutor(max_workers=max_workers, initializer=tqdm_class.set_lock,\n\u001b[1;32m     50\u001b[0m                           initargs=(lk,)) as ex:\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0miterables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1167\u001b[0m         \u001b[0;31m# (note: keep this check outside the loop for performance)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1170\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    619\u001b[0m                     \u001b[0;31m# Careful not to keep a reference to the popped future\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m                         \u001b[0;32myield\u001b[0m \u001b[0m_result_or_cancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    622\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0m_result_or_cancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    451\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "ds_train = select_data(split='train', task='ner', size='testing')\n",
        "\n",
        "# step not needed as can specify keys in batch below\n",
        "dataset = Dataset.from_dict({\n",
        "    'input_ids': ds_train['train']['input_ids'],\n",
        "    'attention_mask': ds_train['train']['attention_mask']\n",
        "}).with_format('torch')\n",
        "\n",
        "# larger batch sizes crash on 12.7 GB CPU\n",
        "dataloader = DataLoader(dataset, batch_size=16)\n",
        "output_dir = f'{path}/models/{model_name}/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xi4tMoQkkAuc"
      },
      "outputs": [],
      "source": [
        "len(ds_train['train']['input_ids'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nzya0-ARWHS5"
      },
      "outputs": [],
      "source": [
        "# test two; success\n",
        "# with torch.no_grad():\n",
        "#     model = LongformerForTokenClassification.from_pretrained(f'{path}/models/{model_name}/model')\n",
        "#     model.config.output_hidden_states=True\n",
        "#     outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "# last_hidden_state = np.array(outputs.hidden_states[-1])\n",
        "# print(last_hidden_state.shape)\n",
        "# np.save(f'{path}/models/{model_name}/last_hidden_state_mini.npy', last_hidden_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYpEM_E21DB3"
      },
      "outputs": [],
      "source": [
        "# stream and save last_hidden_state to avoid memory issues; can optimize further by sending to GPU?\n",
        "with torch.no_grad():\n",
        "    model = LongformerForTokenClassification.from_pretrained(f'{path}/models/{model_name}/model')\n",
        "    model.config.output_hidden_states=True\n",
        "    for i, batch in enumerate(dataloader):\n",
        "        outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
        "        last_hidden_state = np.array(outputs.hidden_states[-1])\n",
        "        file_path = os.path.join(output_dir, f'last_hidden_state_batch_{i}.npy')\n",
        "        np.save(file_path, last_hidden_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ff2LqGlfWqu1"
      },
      "source": [
        "# Create Concatenated / Additive Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRGgq90Hy35s"
      },
      "outputs": [],
      "source": [
        "def create_concat_dataset_full_and_save(model_mask, model_ner, split):\n",
        "    mask_path = f'{path}/models/{model_mask}/hidden_states/{split}/'\n",
        "    ner_path = f'{path}/models/{model_ner}/hidden_states/{split}/'\n",
        "    files = [f for f in os.listdir(mask_path)]\n",
        "\n",
        "    for i, f in enumerate(files):\n",
        "        mpath = os.path.join(mask_path, f)\n",
        "        npath = os.path.join(ner_path, f)\n",
        "        if i == 0:\n",
        "            m_hidden_state = np.load(mpath)\n",
        "            n_hidden_state = np.load(npath)\n",
        "            final_hs = np.concatenate((m_hidden_state, n_hidden_state), axis=-1)\n",
        "        else:\n",
        "            m_hidden_state = np.load(mpath)\n",
        "            n_hidden_state = np.load(npath)\n",
        "            hs = np.concatenate((m_hidden_state, n_hidden_state), axis=-1)\n",
        "            final_hs = np.concatenate((final_hs, hs), axis=0)\n",
        "\n",
        "    print(final_hs.shape)\n",
        "    save_path = f'{path}/data/tab/concatenated_mini/{split}/concat_{i}.npy'\n",
        "    np.save(save_path, final_hs)\n",
        "    print(f'File saved at {save_path}')\n",
        "\n",
        "def create_concat_dataset_and_save(model_mask, model_ner, split):\n",
        "    mask_path = f'{path}/models/{model_mask}/hidden_states/{split}/'\n",
        "    ner_path = f'{path}/models/{model_ner}/hidden_states/{split}/'\n",
        "    files = [f for f in os.listdir(mask_path)]\n",
        "\n",
        "    for i, f in enumerate(files):\n",
        "        mpath = os.path.join(mask_path, f)\n",
        "        npath = os.path.join(ner_path, f)\n",
        "        m_hidden_state = np.load(mpath)\n",
        "        n_hidden_state = np.load(npath)\n",
        "        final_hs = np.concatenate((m_hidden_state, n_hidden_state), axis=-1)\n",
        "\n",
        "        save_path = f'{path}/data/tab/concatenated_mini/{split}/concat_{i}.npy'\n",
        "        np.save(save_path, final_hs)\n",
        "        print(f'File saved at {save_path}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMbzNmawLpyb"
      },
      "outputs": [],
      "source": [
        "# create batched train concatenated vectors\n",
        "split = 'train'\n",
        "create_concat_dataset_and_save(\n",
        "    model_mask='baseline_final_2.5e-5_linear_warmup_11_25',\n",
        "    model_ner='ner_2.5e-5_cosine_warmup_12_02_II',\n",
        "    split=split\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6n24NMjw6sf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc2b42ef-2677-4129-e3b2-3a7f1cda32ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File saved at /content/drive/MyDrive/Colab Notebooks/DATASCI 266/266 project/data/tab/concatenated_mini/val/concat_0.npy\n",
            "File saved at /content/drive/MyDrive/Colab Notebooks/DATASCI 266/266 project/data/tab/concatenated_mini/val/concat_1.npy\n"
          ]
        }
      ],
      "source": [
        "# create test and val data in one file\n",
        "# create_concat_dataset_full_and_save(model_mask='baseline_final_2.5e-5_linear_warmup_11_25',\n",
        "#                                     model_ner='ner_2.5e-5_cosine_warmup_12_02_II',\n",
        "#                                     split='val')\n",
        "\n",
        "# create_concat_dataset_full_and_save(model_mask='baseline_final_2.5e-5_linear_warmup_11_25',\n",
        "#                                     model_ner='ner_2.5e-5_cosine_warmup_12_02_II',\n",
        "#                                     split='test')\n",
        "\n",
        "# split = 'val'\n",
        "# create_concat_dataset_and_save(\n",
        "#     model_mask='baseline_final_2.5e-5_linear_warmup_11_25',\n",
        "#     model_ner='ner_2.5e-5_cosine_warmup_12_02_II',\n",
        "#     split=split\n",
        "# )\n",
        "\n",
        "# split = 'test'\n",
        "# create_concat_dataset_and_save(\n",
        "#     model_mask='baseline_final_2.5e-5_linear_warmup_11_25',\n",
        "#     model_ner='ner_2.5e-5_cosine_warmup_12_02_II',\n",
        "#     split=split\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWgycfXwyNO5",
        "outputId": "3f69936f-335f-4781-987c-97e84544f1fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File saved at /content/drive/MyDrive/Colab Notebooks/DATASCI 266/266 project/data/tab/concatenated_mini/test/concat_0.npy\n",
            "File saved at /content/drive/MyDrive/Colab Notebooks/DATASCI 266/266 project/data/tab/concatenated_mini/test/concat_1.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_concat_dataset_full_and_save(model_mask, model_ner, split):\n",
        "    mask_path = f'{path}/models/{model_mask}/hidden_states/{split}/'\n",
        "    ner_path = f'{path}/models/{model_ner}/hidden_states/{split}/'\n",
        "    files = [f for f in os.listdir(mask_path)]\n",
        "\n",
        "    for i, f in enumerate(files):\n",
        "        mpath = os.path.join(mask_path, f)\n",
        "        npath = os.path.join(ner_path, f)\n",
        "        if i == 0:\n",
        "            m_hidden_state = np.load(mpath)\n",
        "            n_hidden_state = np.load(npath)\n",
        "            final_hs = np.concatenate((m_hidden_state, n_hidden_state), axis=-1)\n",
        "        else:\n",
        "            m_hidden_state = np.load(mpath)\n",
        "            n_hidden_state = np.load(npath)\n",
        "            hs = np.concatenate((m_hidden_state, n_hidden_state), axis=-1)\n",
        "            final_hs = np.concatenate((final_hs, hs), axis=0)\n",
        "\n",
        "    print(final_hs.shape)\n",
        "    save_path = f'{path}/data/tab/concatenated_mini/{split}/concat_{i}.npy'\n",
        "    np.save(save_path, final_hs)\n",
        "    print(f'File saved at {save_path}')"
      ],
      "metadata": {
        "id": "bOBwav2_QcLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# one-off\n",
        "# model_mask='baseline_final_2.5e-5_linear_warmup_11_25'\n",
        "# model_ner='ner_2.5e-5_cosine_warmup_12_02_II'\n",
        "# mask_path = f'{path}/models/{model_mask}/hidden_states/train/train_last_hidden_state_batch_4.npy'\n",
        "# ner_path = f'{path}/models/{model_ner}/hidden_states/train/train_last_hidden_state_batch_4.npy'\n",
        "# m_hidden_state = np.load(mask_path)\n",
        "# n_hidden_state = np.load(ner_path)\n",
        "# final_hs = np.concatenate((m_hidden_state, n_hidden_state), axis=-1)\n",
        "# np.save(f'{path}/data/tab/concatenated_mini/train/concat_4.npy', final_hs)"
      ],
      "metadata": {
        "id": "gwI9BHTHQj9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMuMiva2WzuL"
      },
      "outputs": [],
      "source": [
        "# add_hidden_state = hidden_state_1 + hidden_state_2\n",
        "# print(add_hidden_state.shape)\n",
        "# print(hidden_state_1[0][0][:20])\n",
        "# print(hidden_state_2[0][0][:20])\n",
        "# print(add_hidden_state[0][0][:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrArOPunAE2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "066a7c76-fbd0-4c41-a114-0597047a6f15"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'ner_tags', 'mask_tags', 'text_spans', 'tokens', 'text', 'input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 400\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# grab example labels\n",
        "ds_train = select_data(split='train', task='mask', size='mini')\n",
        "# example_labels = torch.tensor(ds_train['train']['labels'][:32])\n",
        "ds_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1hTmbiHzN63"
      },
      "outputs": [],
      "source": [
        "labels = ds_train['train']['labels']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fe-d6Ctz6LJ3"
      },
      "outputs": [],
      "source": [
        "def get_labels_for_batch(labels, batch_size=32):\n",
        "    start_idx = 0\n",
        "    end_idx = batch_size\n",
        "    batch_labels = {}\n",
        "    for i in range(math.ceil(len(labels) / 32)):\n",
        "        batch_labels[i] = labels[start_idx: end_idx]\n",
        "        start_idx = end_idx\n",
        "        end_idx += batch_size\n",
        "\n",
        "    return batch_labels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_labels = get_labels_for_batch(labels, batch_size=32)\n",
        "batch_labels.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZ2u6hOwCrbO",
        "outputId": "4c19dec1-de20-425c-9a38-567710e3eaff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRAEthOx1ka4",
        "collapsed": true,
        "outputId": "0e538fb9-6ffc-47fb-fc4e-f6eb1bc92e38"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "len(batch_labels[12])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXNYnWsjXGSd"
      },
      "source": [
        "# Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5WLZ7lpXHsN"
      },
      "outputs": [],
      "source": [
        "# functions\n",
        "# metrics\n",
        "def compute_metrics(p):\n",
        "    seqeval = evaluate.load('seqeval')\n",
        "\n",
        "    predictions, labels = p\n",
        "    predictions = predictions[0] # outcoming dim is (1, 32, 4096, 7) instead of (32, 4096, 7)\n",
        "    labels = labels[0] # outcoming dim is (1, 32, 4096) instead of (32, 4096)\n",
        "    # print(predictions.shape)\n",
        "    # print(labels.shape)\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    label_list = ['O', 'B-NO_MASK', 'I-NO_MASK', 'B-DIRECT', 'I-DIRECT', 'B-QUASI', 'I-QUASI']\n",
        "    true_predictions = [\n",
        "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    results = seqeval.compute(predictions=true_predictions, references=true_labels, zero_division=1)\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"seqeval_acc\": results[\"overall_accuracy\"],\n",
        "    }\n",
        "\n",
        "def count_trainable_parameters(model):\n",
        "    # Get the trainable parameters of the model\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    return trainable_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgejBPmhXLF5"
      },
      "outputs": [],
      "source": [
        "# classes\n",
        "class ConcatTokenClassificationModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            # nn.Linear(hidden_dim, hidden_dim),\n",
        "            # nn.Relu(),\n",
        "            nn.Linear(hidden_dim, num_classes)\n",
        "        )\n",
        "        self.loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, input_ids, labels=None):\n",
        "        logits = self.linear_relu_stack(input_ids)\n",
        "\n",
        "        if labels is not None:\n",
        "            loss = self.loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
        "            return {\"logits\": logits, \"loss\": loss}\n",
        "\n",
        "        return {\"logits\": logits}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_labels_for_batch(labels, batch_size=32):\n",
        "    start_idx = 0\n",
        "    end_idx = batch_size\n",
        "    batch_labels = {}\n",
        "    for i in range(math.ceil(len(labels) / 32)):\n",
        "        batch_labels[i] = labels[start_idx: end_idx]\n",
        "        start_idx = end_idx\n",
        "        end_idx += batch_size\n",
        "\n",
        "    return batch_labels"
      ],
      "metadata": {
        "id": "lS_T-YmQC-pY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSv2G4CbtynQ"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import IterableDataset\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "class FileStreamingDataset(IterableDataset):\n",
        "    def __init__(self, split, labels, process_function, batch_size=32, num_files=None):\n",
        "        super(FileStreamingDataset).__init__()\n",
        "        self.folder = f'{path}/data/tab/concatenated_mini/{split}'\n",
        "        if num_files == None:\n",
        "            self.num_files = len(os.listdir(file_path))\n",
        "        else:\n",
        "            self.num_files = num_files\n",
        "        self.labels = labels\n",
        "        self.process_function = process_function\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __iter__(self):\n",
        "        batch_count = 0\n",
        "        for x in range(self.num_files):\n",
        "            file_path = os.path.join(self.folder, f'concat_{x}.npy')\n",
        "            hidden_states = np.load(file_path)\n",
        "            # print(f'Processing file: {file_path}, {hidden_states.shape}')\n",
        "\n",
        "            # Process the data and yield the batches of individual samples\n",
        "            num_samples = hidden_states.shape[0]\n",
        "            for i in range(0, num_samples, self.batch_size):\n",
        "                batch_hidden_states = hidden_states[i:i+self.batch_size]\n",
        "                batch_labels = self.labels[x]\n",
        "                # print(f'Batch labels: {len(batch_labels)}')\n",
        "\n",
        "                # Process the batch into the desired format\n",
        "                batch = self.process_function(batch_hidden_states, batch_labels)\n",
        "\n",
        "                batch_count += 1\n",
        "                # print(f'Yielding batch: {batch_count}, {batch_hidden_states.shape}')\n",
        "                yield batch\n",
        "\n",
        "def process_line(hidden_states, labels):\n",
        "    # Ensure inputs are numpy arrays and properly formatted\n",
        "    if not isinstance(hidden_states, np.ndarray):\n",
        "        raise ValueError(f\"Expected hidden_states as np.ndarray, got {type(hidden_states)}\")\n",
        "    if not isinstance(labels, list):\n",
        "        raise ValueError(f\"Expected labels as list, got {type(labels)}\")\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    input_ids = torch.tensor(hidden_states, dtype=torch.float32)\n",
        "    label_tensor = torch.tensor(labels, dtype=torch.int64)  # CrossEntropyLoss expects int64 for labels\n",
        "\n",
        "    # Return a dictionary suitable for Trainer\n",
        "    return {'input_ids': input_ids, 'labels': label_tensor}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def pad_collate(batch):\n",
        "#     max_batch_size = max(item['input_ids'].size(0) for item in batch)\n",
        "#     padded_batch = {\n",
        "#         'input_ids': torch.stack([torch.cat([item['input_ids'],\n",
        "#                                              torch.zeros(max_batch_size - item['input_ids'].size(0),\n",
        "#                                                          item['input_ids'].size(1),\n",
        "#                                                          item['input_ids'].size(2)])\n",
        "#                                   for item in batch]),\n",
        "#         'labels': torch.cat([item['labels'],\n",
        "#                              torch.full((max_batch_size - item['labels'].size(0),),\n",
        "#                                         fill_value=-100)])  # Use -100 for ignored labels\n",
        "#     }\n",
        "#     return padded_batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "afoEA0yU69PN",
        "outputId": "942b6188-3197-4b6b-d33d-f1f818ef2aaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "closing parenthesis '}' does not match opening parenthesis '[' on line 4 (<ipython-input-60-13e4f2fb7778>, line 12)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-60-13e4f2fb7778>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    }\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m closing parenthesis '}' does not match opening parenthesis '[' on line 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize train dataset\n",
        "ds_train = select_data(split='train', task='mask', size='mini')\n",
        "labels = ds_train['train']['labels']\n",
        "batch_labels = get_labels_for_batch(labels, batch_size=32)\n",
        "print(batch_labels.keys())\n",
        "\n",
        "# split = 'train'\n",
        "# file_path = f'{path}/data/tab/concatenated_mini/{split}'\n",
        "# files = [f'{file_path}/{f}' for f in os.listdir(file_path)]\n",
        "\n",
        "streaming_dataset = FileStreamingDataset(split='train',\n",
        "                                         labels=batch_labels,\n",
        "                                         process_function=process_line,\n",
        "                                         num_files=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfRTc_nADUct",
        "outputId": "cd6be8f3-c2ba-48ac-c08e-732ecd531460"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# eval_hidden_states = np.load(f'{path}/data/tab/concatenated_mini/val/concat_1.npy')\n",
        "ds_val = select_data(split='val', task='mask', size='mini')\n",
        "labels = ds_val['train']['labels']\n",
        "eval_batch_labels = get_labels_for_batch(labels, batch_size=32)\n",
        "print(eval_batch_labels.keys())\n",
        "\n",
        "# # val\n",
        "# split = 'val'\n",
        "# file_path = f'{path}/data/tab/concatenated_mini/{split}'\n",
        "# val_files = [f'{file_path}/{f}' for f in os.listdir(file_path)]\n",
        "\n",
        "eval_streaming_dataset = FileStreamingDataset(split='val',\n",
        "                                              labels=eval_batch_labels,\n",
        "                                              process_function=process_line,\n",
        "                                              num_files=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ja0ebc_1wcw9",
        "outputId": "e467a05d-0577-4f3a-854c-6b3b1500dbb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys([0, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# eval_hidden_states = np.load(f'{path}/data/tab/concatenated_mini/val/concat_1.npy')\n",
        "# ds_val = select_data(split='val', task='mask', size='mini')\n",
        "# eval_labels = ds_val['train']['labels']"
      ],
      "metadata": {
        "id": "K0C9shzoJ14i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# eval_dataset = Dataset.from_dict({\n",
        "#     'input_ids': torch.tensor(np.load(f'{path}/data/tab/concatenated_mini/val/concat_1.npy'), dtype=torch.float32),\n",
        "#     'labels': torch.tensor(eval_labels, dtype=torch.int64)\n",
        "# })"
      ],
      "metadata": {
        "id": "gFw_tmlAKgyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0HBhbfPXP8O"
      },
      "outputs": [],
      "source": [
        "seq_length = 4096\n",
        "input_dim = 1536\n",
        "hidden_dim = 512\n",
        "num_classes = 7\n",
        "\n",
        "model = ConcatTokenClassificationModel(input_dim=input_dim,\n",
        "                                       hidden_dim=hidden_dim,\n",
        "                                       num_classes=num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset = Dataset.from_dict({\n",
        "#     'input_ids': torch.tensor(np.load('/content/drive/MyDrive/Colab Notebooks/DATASCI 266/266 project/data/tab/concatenated_mini/train/concat_0.npy')),\n",
        "#     'labels': torch.tensor(ds_train['train']['labels'][:32])\n",
        "# })"
      ],
      "metadata": {
        "id": "GdsOklYCeEHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# type(dataset['input_ids'])"
      ],
      "metadata": {
        "id": "53aMWzgAnruR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hz1gQhhQWKB3",
        "outputId": "0b5ba2d7-1ddf-4109-8bed-f14a6dc76eb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ConcatTokenClassificationModel(\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=1536, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.1, inplace=False)\n",
            "    (3): Linear(in_features=512, out_features=7, bias=True)\n",
            "  )\n",
            "  (loss_fn): CrossEntropyLoss()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # TrainingArguments with eval; oom issues\n",
        "# model_name = 'concat_base_1e-4_test'\n",
        "\n",
        "# batch_size = 32\n",
        "# num_train_epochs = 5\n",
        "# max_steps = (128 // batch_size) * num_train_epochs\n",
        "\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir=f'{path}/models/{model_name}/results',\n",
        "#     eval_strategy='epoch',\n",
        "#     save_strategy='epoch',\n",
        "#     logging_strategy='epoch',\n",
        "#     save_total_limit=2,\n",
        "#     load_best_model_at_end=True,\n",
        "#     save_only_model=True,\n",
        "#     metric_for_best_model='eval_loss',\n",
        "#     per_device_train_batch_size=32,\n",
        "#     per_device_eval_batch_size=32,\n",
        "#     greater_is_better=False,\n",
        "#     learning_rate=1e-4,\n",
        "#     max_steps=max_steps, # overrides num_train_epochs\n",
        "#     # num_train_epochs=num_train_epochs,\n",
        "#     report_to='none')\n",
        "\n",
        "# trainer = Trainer(\n",
        "#     model=model,\n",
        "#     args=training_args,\n",
        "#     train_dataset=streaming_dataset,\n",
        "#     eval_dataset=eval_streaming_dataset,\n",
        "#     compute_metrics=compute_metrics\n",
        "# )\n",
        "\n",
        "# trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "UUUtca7pJinf",
        "outputId": "d832230e-d94b-44eb-dcde-332306a57ccd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 2/20 : < :, Epoch 0.05/9223372036854775807]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "      <th>Seqeval Acc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.910500</td>\n",
              "      <td>1.550794</td>\n",
              "      <td>0.048025</td>\n",
              "      <td>0.075220</td>\n",
              "      <td>0.058622</td>\n",
              "      <td>0.776915</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 4096, 7)\n",
            "(32, 4096)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TrainingArguments w/o eval\n",
        "model_name = 'concat_base_0.01_test'\n",
        "\n",
        "batch_size = 32\n",
        "num_train_epochs = 5\n",
        "max_steps = (128 // batch_size) * num_train_epochs\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=f'{path}/models/{model_name}/results',\n",
        "    # eval_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    logging_strategy='epoch',\n",
        "    save_total_limit=2,\n",
        "    # load_best_model_at_end=True,\n",
        "    # save_only_model=True,\n",
        "    # metric_for_best_model='eval_loss',\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    greater_is_better=False,\n",
        "    learning_rate=0.01,\n",
        "    max_steps=max_steps, # overrides num_train_epochs\n",
        "    # num_train_epochs=num_train_epochs,\n",
        "    report_to='none')\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=streaming_dataset,\n",
        "    # eval_dataset=eval_streaming_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 772
        },
        "id": "Wm_u7kz-CkX6",
        "outputId": "03f33cd6-7e59-4cee-ac0f-48f7ba77d39a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [20/20 13:16, Epoch 19/9223372036854775807]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.851300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>9.503200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>8.370700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>5.664400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.463900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>14.099100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>20.343000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>23.840500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>20.829000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>17.385800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>12.980700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>7.567300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>2.357000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>2.307400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>2.644700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>2.740100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>2.692100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>2.570200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>2.417000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.276900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=20, training_loss=8.245213794708253, metrics={'train_runtime': 830.534, 'train_samples_per_second': 0.771, 'train_steps_per_second': 0.024, 'total_flos': 0.0, 'train_loss': 8.245213794708253, 'epoch': 19.05})"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate(eval_dataset=streaming_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sA_mXRkOzqre",
        "outputId": "6b95c516-3c8c-45fe-a654-0df36d96723f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 2.189661741256714,\n",
              " 'eval_precision': 0.6052631578947368,\n",
              " 'eval_recall': 0.020063972084908403,\n",
              " 'eval_f1': 0.03884041654939488,\n",
              " 'eval_seqeval_acc': 0.793350928369681,\n",
              " 'eval_runtime': 39.3866,\n",
              " 'eval_samples_per_second': 0.102,\n",
              " 'eval_steps_per_second': 0.025,\n",
              " 'epoch': 19.05}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate(eval_dataset=eval_streaming_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ub0-iKtnFFQh",
        "outputId": "9bf19e94-691a-42cf-cda5-204319c4a6cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 1.1053968667984009,\n",
              " 'eval_precision': 0.9310344827586207,\n",
              " 'eval_recall': 0.03275705186533212,\n",
              " 'eval_f1': 0.06328743041312629,\n",
              " 'eval_seqeval_acc': 0.7734955293326232,\n",
              " 'eval_runtime': 8.5946,\n",
              " 'eval_samples_per_second': 0.116,\n",
              " 'eval_steps_per_second': 0.116,\n",
              " 'epoch': 19.05}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test pipeline\n",
        "# predictions, labels, metrics = trainer.predict(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "LvDJV3pwzMPo",
        "outputId": "1874a2cb-f141-4f01-9b69-db2c1b525055"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# predictions.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYmI24qf1WwQ",
        "outputId": "51b3afb9-0f7a-4493-b0eb-0d642131e991"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32, 4096, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CiLIDl8DXanj"
      },
      "outputs": [],
      "source": [
        "# save hf/pytorch model\n",
        "trainer.save_model(f'{path}/models/{model_name}/model')\n",
        "# did not save tokenizer as already tokenized; load default longformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHNEocSzXcV0"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tP4KyMXXeik"
      },
      "outputs": [],
      "source": [
        "ds_test = select_data(split='test', task=task, size=size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24T7YkUCXfhR"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate(eval_dataset=ds_test['train'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpZ_D7AHXgiV"
      },
      "outputs": [],
      "source": [
        "predictions, labels, metrics = trainer.predict(ds_test['train'])\n",
        "print(f\"Metrics: {metrics}\")\n",
        "print(predictions[0])\n",
        "print(labels[0])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "u69WmUnfWAyj",
        "Ff2LqGlfWqu1"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}